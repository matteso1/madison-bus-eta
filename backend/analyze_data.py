#!/usr/bin/env python3
"""
Quick analysis of collected Madison Metro data for ML readiness
"""

import pandas as pd
import glob
import os
from datetime import datetime

def analyze_data():
    print('üîç ANALYZING MADISON METRO DATA FOR ML READINESS')
    print('=' * 60)

    # Load sample data
    pred_files = glob.glob('collected_data/predictions_*.csv')[:10]
    veh_files = glob.glob('collected_data/vehicles_*.csv')[:10]

    print(f'üìÅ Found {len(pred_files)} prediction files (sampling first 10)')
    print(f'üìÅ Found {len(veh_files)} vehicle files (sampling first 10)')

    # Analyze prediction data
    if pred_files:
        pred_dfs = []
        for f in pred_files:
            try:
                df = pd.read_csv(f)
                pred_dfs.append(df)
            except Exception as e:
                print(f"Error loading {f}: {e}")
                continue
        
        if pred_dfs:
            pred_data = pd.concat(pred_dfs, ignore_index=True)
            print(f'\nüìä PREDICTION DATA ANALYSIS:')
            print(f'  ‚Ä¢ Total records: {len(pred_data):,}')
            print(f'  ‚Ä¢ Unique routes: {pred_data["rt"].nunique()}')
            print(f'  ‚Ä¢ Unique stops: {pred_data["stpid"].nunique()}')
            print(f'  ‚Ä¢ Time span: {pred_data["collection_timestamp"].min()} to {pred_data["collection_timestamp"].max()}')
            
            # Check prediction quality
            valid_preds = pred_data[pred_data['prdctdn'].notna()]
            print(f'  ‚Ä¢ Valid predictions: {len(valid_preds):,} ({len(valid_preds)/len(pred_data)*100:.1f}%)')
            
            # Check delay data
            delay_data = pred_data[pred_data['dly'].notna()]
            delay_rate = delay_data['dly'].mean() if len(delay_data) > 0 else 0
            print(f'  ‚Ä¢ Delay rate: {delay_rate:.1%}')
            
            # Check prediction countdown distribution
            if len(valid_preds) > 0:
                countdown_stats = valid_preds['prdctdn'].value_counts().head(10)
                print(f'  ‚Ä¢ Top prediction values: {dict(countdown_stats.head(5))}')

    # Analyze vehicle data
    if veh_files:
        veh_dfs = []
        for f in veh_files:
            try:
                df = pd.read_csv(f)
                veh_dfs.append(df)
            except Exception as e:
                print(f"Error loading {f}: {e}")
                continue
        
        if veh_dfs:
            veh_data = pd.concat(veh_dfs, ignore_index=True)
            print(f'\nüöó VEHICLE DATA ANALYSIS:')
            print(f'  ‚Ä¢ Total records: {len(veh_data):,}')
            print(f'  ‚Ä¢ Unique vehicles: {veh_data["vid"].nunique()}')
            print(f'  ‚Ä¢ Unique routes: {veh_data["rt"].nunique()}')
            print(f'  ‚Ä¢ Time span: {veh_data["collection_timestamp"].min()} to {veh_data["collection_timestamp"].max()}')
            
            # Check data quality
            valid_speed = veh_data[veh_data['spd'].notna()]
            print(f'  ‚Ä¢ Valid speed data: {len(valid_speed):,} ({len(valid_speed)/len(veh_data)*100:.1f}%)')
            
            # Check delay data
            delay_data = veh_data[veh_data['dly'].notna()]
            delay_rate = delay_data['dly'].mean() if len(delay_data) > 0 else 0
            print(f'  ‚Ä¢ Delay rate: {delay_rate:.1%}')
            
            # Speed statistics
            if len(valid_speed) > 0:
                print(f'  ‚Ä¢ Average speed: {valid_speed["spd"].mean():.1f} mph')
                print(f'  ‚Ä¢ Speed range: {valid_speed["spd"].min():.1f} - {valid_speed["spd"].max():.1f} mph')

    # ML Readiness Assessment
    print(f'\nüéØ ML READINESS ASSESSMENT:')
    print('=' * 60)
    
    # Calculate total data volume
    total_files = len(glob.glob('collected_data/*.csv'))
    print(f'üìà DATA VOLUME:')
    print(f'  ‚Ä¢ Total CSV files: {total_files:,}')
    print(f'  ‚Ä¢ Estimated total records: {total_files * 100:,} (rough estimate)')
    
    # Assess ML readiness
    print(f'\nü§ñ ML MODEL READINESS:')
    
    if total_files > 1000:
        print('  ‚úÖ EXCELLENT: More than 1,000 data files')
        print('  ‚úÖ Sufficient for complex ML models (Random Forest, XGBoost)')
        print('  ‚úÖ Good for deep learning models')
    elif total_files > 500:
        print('  ‚úÖ GOOD: More than 500 data files')
        print('  ‚úÖ Sufficient for standard ML models')
        print('  ‚ö†Ô∏è  Consider collecting more for deep learning')
    elif total_files > 100:
        print('  ‚ö†Ô∏è  MODERATE: More than 100 data files')
        print('  ‚úÖ Sufficient for simple ML models (Linear Regression)')
        print('  ‚ö†Ô∏è  Limited for complex models')
    else:
        print('  ‚ùå LIMITED: Less than 100 data files')
        print('  ‚ö†Ô∏è  Consider collecting more data')
    
    print(f'\nüí° RECOMMENDATIONS:')
    print('=' * 60)
    
    if total_files > 1000:
        print('üéâ YOUR DATA IS READY FOR ML!')
        print('  ‚Ä¢ You have excellent data volume')
        print('  ‚Ä¢ Can train multiple model types')
        print('  ‚Ä¢ Good for production deployment')
        print('  ‚Ä¢ Consider running ML pipeline now!')
    elif total_files > 500:
        print('üëç GOOD TO GO FOR BASIC ML!')
        print('  ‚Ä¢ Sufficient for most ML models')
        print('  ‚Ä¢ Can start training now')
        print('  ‚Ä¢ Continue collecting for better accuracy')
    else:
        print('‚è≥ CONSIDER COLLECTING MORE DATA')
        print('  ‚Ä¢ Current data is limited')
        print('  ‚Ä¢ Can start with simple models')
        print('  ‚Ä¢ Continue collection for better results')
    
    print(f'\nüöÄ NEXT STEPS:')
    print('  1. Run the ML training pipeline')
    print('  2. Evaluate model performance')
    print('  3. Deploy best model to API')
    print('  4. Continue data collection for improvement')

if __name__ == "__main__":
    analyze_data()

